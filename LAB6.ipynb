{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqOizU45T1+nFOqltDRWPZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import csv\n","!pip install gymnasium\n","import gymnasium as gym\n","from tqdm import tqdm\n","import pickle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGnVuS2p9Qh8","executionInfo":{"status":"ok","timestamp":1686245334483,"user_tz":-180,"elapsed":3365,"user":{"displayName":"Ayşe Nur Gümüş","userId":"04570087753131263407"}},"outputId":"396001c2-bfa3-4c65-b741-e37fc6e42abc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.28.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n","Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"]}]},{"cell_type":"code","source":["# number of buckets for position and velocity\n","pos_bucket_number = 20\n","vel_bucket_number = 20"],"metadata":{"id":"hau_3v0QDbVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These bucket numbers are used to divide the observation space into smaller partitions or bins, representing different states. By dividing the observation space into smaller buckets, learning algorithms or strategies can better understand the states and adjust their actions accordingly. Smaller buckets help capture the state of the game in more detail, allowing for more precise movements and strategies to be developed."],"metadata":{"id":"b-kpLM7m9THs"}},{"cell_type":"code","source":["action_space = [0, 1, 2]"],"metadata":{"id":"AQJgb0x89Xcq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" There are 3 discrete deterministic actions:\n"," \n","  0: Accelerate to the left\n","\n"," 1: Don’t accelerate\n","\n","2: Accelerate to the right"],"metadata":{"id":"AIOgmU9--Cee"}},{"cell_type":"code","source":["pos_space = np.linspace(-1.2, 0.6, pos_bucket_number)\n","vel_space = np.linspace(-0.07, 0.07, vel_bucket_number)"],"metadata":{"id":"9ibDN5Nx-L36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bins for position and velocity\n","\n","position is between [-1.2, 0.6]\n","\n","and velocity [-0.07, 0.07]"],"metadata":{"id":"d_Y61SOX-nms"}},{"cell_type":"code","source":["def get_state(observation):\n","    # Car Position, Car Velocity\n","    (pos, vel) = observation\n","    # return bins of current observation state\n","    pos_bin = int(np.digitize(pos, pos_space))\n","    vel_bin = int(np.digitize(vel, vel_space))\n","\n","    return (pos_bin, vel_bin)"],"metadata":{"id":"eLsAEN4b-woO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By partitioning continuous observation data (such as position and velocity) into bins, we can obtain a discrete state representation. This state representation facilitates the understanding of the environment by reinforcement learning algorithms and enables the agent to make better decisions."],"metadata":{"id":"FWGVEkoX_3EG"}},{"cell_type":"code","source":["# find max action from Q (Q is a dict)\n","def max_action(Q, state, actions=action_space):\n","    values = np.array([Q[state, a] for a in actions])\n","    action = np.argmax(values)\n","\n","    return action"],"metadata":{"id":"PpoW1FA9_4EQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The max_action() function is used to find the action with the highest value in the Q-table. This allows reinforcement learning algorithms to select the best action based on the current state."],"metadata":{"id":"K7KhPALSALhT"}},{"cell_type":"code","source":["# This function is a tool used to track and visualize the performance of a reinforcement learning algorithm.\n","def plot(scores, epsilons, num_of_episodes):\n","    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 12), sharex=True)\n","\n","    ax1.plot(scores, color='blue')\n","    ax1.set_xlabel('Episode', fontsize=16)\n","    ax1.set_ylabel('Score', color='blue', fontsize=16)\n","    ax1.tick_params(axis='y', labelcolor='blue', labelsize=14)\n","    ax1.set_ylim(-1000, 0)\n","    ax1.set_yticks(range(-1000, 1, 100))\n","    ax1.set_xlim(0, num_of_episodes)\n","    ax1.set_xticks(np.arange(0, num_of_episodes, 100))\n","    ax1.grid(alpha=0.4)\n","\n","    ax2.plot(epsilons, color='red', linewidth=3)\n","    ax2.set_ylabel('Epsilon', color='red', fontsize=16)\n","    ax2.tick_params(axis='y', labelcolor='red', labelsize=14)\n","    ax2.set_ylim(0, 1)\n","    ax2.grid(alpha=0.4)\n","\n","    mean_scores = [np.mean(scores[max(0, i - 99):(i + 1)]) for i in range(len(scores))]\n","\n","    ax3 = ax1.twinx()\n","    ax3.plot(mean_scores, color='green', linewidth=3)\n","    ax3.set_ylabel('Mean score (last 100 episodes)', color='green', fontsize=16)\n","    ax3.tick_params(axis='y', labelcolor='green', labelsize=14)\n","    ax3.set_ylim(-1000, 0)\n","    ax3.set_yticks(range(-1000, 1, 100))\n","    ax3.spines['right'].set_visible(False)\n","    ax3.spines['left'].set_visible(False)\n","    ax3.yaxis.set_label_position('right')\n","    ax3.yaxis.tick_right()\n","\n","    plt.title('num_of_episodes = ' + str(num_of_episodes), fontsize=20, color='green')\n","\n","    plt.tight_layout()\n","    plt.savefig('scores_mcc.png')\n","    plt.show()"],"metadata":{"id":"v19FJ8B3ApHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    num_of_episodes = 5000 #the total number of episodes (iterations) the agent will run during the learning process.\n","    num_of_steps_per_episode = 1000 #represents the maximum number of steps the agent can take within each episode. If the agent doesn't reach the goal or terminate before reaching this step limit, the episode ends\n","    alpha = 0.1  # is the learning rate parameter used in the Q-learning algorithm. It determines the weight given to the new information obtained from each learning step.\n","    gamma = 0.9  # is the discount factor parameter used in the Q-learning algorithm. It determines the importance of future rewards compared to immediate rewards. A higher value of gamma gives more weight to future rewards.\n","    eps = 1.  # the exploration parameter or epsilon, represents the probability of an agent taking a random action"],"metadata":{"id":"t4BAKqwnF1MF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('MountainCar-v0', max_episode_steps=num_of_steps_per_episode)"],"metadata":{"id":"wxREJfvfFXms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the Mountain Car environment, a car tries to climb a hill but doesn't have enough power to do so on its own. The car needs to apply the gas at the right time and control the acceleration correctly to reach the top of the hill. The environment is defined by the car's current position, velocity, and three possible actions to choose from (left, stay, right).\n","\n","This line creates the environment and assigns it to the env variable, making it ready to perform actions and interact with the environment."],"metadata":{"id":"AqyUuCX2FvCb"}},{"cell_type":"code","source":["    states = list()\n","    Q = {}\n","\n","    scores = np.zeros(num_of_episodes)\n","    epsilons = np.zeros(num_of_episodes)\n","\n","    for position in range(pos_bucket_numer + 1):\n","        for velocity in range(vel_bucket_number + 1):\n","            states.append((position, velocity))\n","\n","    for state in states:\n","        for action in action_space:\n","            Q[state, action] = 0"],"metadata":{"id":"5KjWLWeSJxH7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This loop initializes the Q-values in the dictionary Q with a value of 0 for each state-action pair.\n","\n","This code snippet prepares an empty dictionary Q to store the Q-values and creates the list of possible states in the environment along with their initial values."],"metadata":{"id":"LRY8k3-aJyqg"}},{"cell_type":"code","source":[" # create progress bar\n","progress_bar = tqdm(total=num_of_episodes, desc='Learning')"],"metadata":{"id":"l1jy7uSVKCYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This progress bar is a tool used to track the progress of the learning process in the reinforcement learning algorithm."],"metadata":{"id":"5xY5NQGPLMzs"}},{"cell_type":"code","source":["    # learning loop\n","    for episode in range(num_of_episodes):\n","        terminated = False\n","        truncated = False\n","\n","        obs, _ = env.reset()\n","        state = get_state(obs)\n","\n","        score = 0\n","\n","        while not (terminated or truncated):\n","            if np.random.random() < eps:\n","                action = np.random.choice(action_space)\n","            else:\n","                action = max_action(Q, state)\n","\n","            obs_new, reward, terminated, truncated, info = env.step(action)\n","            score += reward\n","\n","            # calculate Q\n","            state_new = get_state(obs_new)\n","            action_new = max_action(Q, state)\n","            Q[state, action] = Q[state, action] + alpha*(reward + gamma*Q[state_new, action_new] - Q[state, action])\n","\n","            state = state_new"],"metadata":{"id":"Jy0oHdm9dRF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The terminated** variable represents the condition where the car has reached the goal and the episode is completed successfully. If the car reaches the top of the hill and accomplishes the task, the terminated value becomes True.\n","\n","**The truncated** variable is used when the episode has a maximum step limit (num_of_steps_per_episode). If the car fails to reach the goal or satisfy other termination conditions within the maximum step limit, the truncated value becomes True. In this case, the episode ends, but it is not considered a successful completion."],"metadata":{"id":"-Vy3B8-QLkzm"}},{"cell_type":"markdown","source":["Q[state_new, action_new] - Q[state, action]\n","\n","we had calculated the error "],"metadata":{"id":"3YaAw8loODfj"}},{"cell_type":"code","source":["        # decrease epsilon over time (in halfway selection strategy will be almost entirely greedy)\n","        eps = eps - 2/num_of_episodes if eps > 0.01 else 0."],"metadata":{"id":"ccnpnFTQchKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It decreases the value of epsilon over time. This allows the agent to focus more on making accurate predictions by reducing the exploration rate. Here, the epsilon value is decreased by 2/num_of_episodes in each iteration. However, if the epsilon value is less than 0.01, it is not decreased further and the minimum value of 0.01 is used."],"metadata":{"id":"oBViFeiycnKJ"}},{"cell_type":"code","source":["scores[episode] = score\n","epsilons[episode] = eps\n"],"metadata":{"id":"dXg5yUkdd3RZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It saves the score and epsilon values obtained for each episode in the 'scores' and 'epsilons' arrays, respectively. These arrays can be used to track the progress of the learning process and visualize the results."],"metadata":{"id":"w5Vlz-0dd5FH"}},{"cell_type":"code","source":["        # per one hundred episodes calculate mean score,\n","        # which is shown on the progress bar\n","        if episode % 100 == 0:\n","            # mean of last 100 episodes\n","            mean = np.mean(scores[max(0, episode - 100):(episode + 1)])\n"],"metadata":{"id":"u71C3P9NeJL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["        # Update the progress bar\n","        progress_bar.set_postfix(epsilon=f'{eps:.2f}',\n","                                 score=str(int(score)),\n","                                 mean_score=str(int(mean)))\n","        progress_bar.update(1)\n","\n","    env.close()\n","\n","    plot(scores, epsilons, num_of_episodes)"],"metadata":{"id":"Cr-SQapIeagX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It updates and advances the progress bar. \n","\n","After completing the learning loop, it closes the environment."],"metadata":{"id":"rB8n4ePievEM"}}]}